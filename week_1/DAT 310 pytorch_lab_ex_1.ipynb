{"cells":[{"cell_type":"markdown","metadata":{"id":"jmpD8moR3hGq"},"source":["<center>\n","    <img src=\"https://www.ucalgary.ca/themes/ucalgary/ucws_theme/images/UCalgary.svg\" width='30%'>\n","</center>\n","\n","\n","[comment]: <> (The following line is for the LECTURE title)\n","<p style=\"text-align:left;\"><font size='6'><b> Deep Learning - Lab </b></font></p>\n","\n","[comment]: <> (The following line is for the TOPIC of the week)\n","<p style=\"text-align:left;\"><font size='4'><b> Pytorch Exercises Part 1 </b></font></p>\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V7j9aaXQ_PWD"},"source":["Let's take a sneak peek at the some digital logic gates for this exercise. \n","\n","\\\n","\n","\n","Given what we know about an MLP's ability to approximate functons that are not linearly seperable we should quite easily be able to solve XOR without backpropogation, and just simply dial in the correct weights and biases.\n","\n","\\\n","\n","Before we head down that line of investigation let's go ahead and build what we might need from scratch. This excersise will have you working in two scenarios:\n","\n","\n","- Scenario one will see us solve the XOR problem with MSE loss, A Recitified Linear Unit in our hidden layer to add non-linearity and an output dimension of 1. \\begin{align}\n","Relu(x) & = max(0,x)\n","\\end{align}  \n","\n","- Scenario two demonstrates that we can also solve the problem with Binary Cross entropy and simply Sigmoid as our output activation without the need for a Rectified Linear Unit.\n","\n","\\begin{align}\n","y=\\dfrac{1}{1+e^{-x}}\n","\\end{align}\n","\n","\n","\n","Both scenarios will then be carried forward to our upcoming lab where we'll begin to add some more bells and whistles.\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"FUG1sKLN_MvI"},"outputs":[],"source":["#Digital logic gates\n","\n","import numpy as np\n","\n","X = np.array([[0, 0], [0, 1], [1,0], [1, 1]])\n","\n","gates = {'OR': np.array([0,1,1,1]),\n","         'AND': np.array([0,0,0,1]),\n","         'XOR': np.array([0,1,1,0])}"]},{"cell_type":"markdown","metadata":{"id":"vlOS4Av9PTyc"},"source":["##Part 1\n","\n","- Convert the input data above from numpy matrices and vectors to tensors.\n","- Convert the XOR ground truths to tensor.\n","- Cast the resultant tensors to float32"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0kelpSOVPXYs"},"outputs":[],"source":["####Your code here###\n","import torch\n","X = torch.from_numpy(X).float()\n","XOR_labels = torch.tensor(gates['XOR'], dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1642617634037,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"_AwP-wqiGoph","outputId":"1773e2ee-780e-4236-bfed-03c12f5bdfff"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 1.],\n","        [1., 0.],\n","        [1., 1.]]) tensor([0., 1., 1., 0.])\n"]}],"source":["print(X, XOR_labels)"]},{"cell_type":"markdown","metadata":{"id":"1R1LEY9o9-VD"},"source":["##Part 2\n","\n","Let's put aside backpropogation for now and create randomly initialsied weights and biases and construct a feedforward network with the following scenarios (see below for further conditions:\n","\n","- A single hidden layer with two hidden neurons for the whole batch, and an output dimension of 1.\n","\n","\\\n","\n","----\n","\n","Conditions:\n","\n","- don't worry about `torch.Variable` for this challenge as we don't need to keep track of gradients etc here. however all operations should be constructed from the ground up with `torch.tensor` etc.\n","\n","- It's also worth bearing in mind that while Pytorch ordinarily takes care of transposing our weights matrix for our hidden layers, we'll need to do this manually below confirm with those. Given this your hidden dense layer should look like:\n","\n","\\begin{align}\n","h= g(xW^T + c)\n","\\end{align}\n","\n","\n","\n","\n","\\\n","\n","---\n","\n","Note that your output should be a vector consisting of four predictions\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1642593485492,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"OdVsAcgNKupZ","outputId":"017fb5c4-0172-430d-d091-e0de08e53d79"},"outputs":[{"data":{"text/html":["\n","        <iframe\n","            width=\"500\"\n","            height=\"300\"\n","            src=\"https://drive.google.com/file/d/12r-yy2lYIpn4v0ULf8jFMDygPyKaNGSj/preview\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7f33e0282090>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import IFrame\n","IFrame(src='https://drive.google.com/file/d/12r-yy2lYIpn4v0ULf8jFMDygPyKaNGSj/preview', width=500, height=300)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423,"status":"ok","timestamp":1642614614962,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"Hpw9us6RONVa","outputId":"7f9beeac-71d5-412f-a533-078ac6b43f46"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 2])\n","torch.Size([4, 1])\n"]}],"source":["####your code here###\n","\n","#functions\n","def feed_forward(X, weights, bias):\n","    \n","    return torch.matmul(X, weights.T) + bias\n","\n","\n","#weights\n","weights_hidden = torch.randn(2, 2)\n","bias_hidden = torch.randn(2,)\n","\n","weights_out = torch.randn(1, 2)\n","bias_out = torch.randn(1,)\n","\n","#number of out nodes, one bias per neuron\n","# print(feed_forward(X, weights, bias))\n","\n","#hidden layer\n","h = feed_forward(X, weights_hidden, bias_hidden)\n","print(h.shape)\n","\n","# #output\n","out = feed_forward(h, weights_out, bias_out)\n","print(out.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"Puqq07nBORE6"},"source":["- A single hidden layer with > 2 hidden neurons for the whole batch, and an output dimension of 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1642594042091,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"SFjqOuY4OPwD","outputId":"dd7420ff-f141-44b0-e4f1-e7322829adef"},"outputs":[{"data":{"text/html":["\n","        <iframe\n","            width=\"500\"\n","            height=\"300\"\n","            src=\"https://drive.google.com/file/d/10ZR_eagfu9peGsuMvIywpRh4KGrQV6ed/preview\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7f33e0279910>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import IFrame\n","IFrame(src='https://drive.google.com/file/d/10ZR_eagfu9peGsuMvIywpRh4KGrQV6ed/preview', width=500, height=300)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1642614618647,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"OfCyrbL0W7JQ","outputId":"6739e3db-bbee-484c-eba3-9f85b1975908"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.1639,  2.1796,  0.3649, -1.1730,  0.1114, -1.1045,  0.3718,  0.9008,\n","          0.0292, -0.6376, -0.3279, -0.4540,  1.4255, -0.7676,  1.4256],\n","        [-0.6668,  0.9973,  0.2383, -1.4899, -0.6074, -1.6783, -0.0960,  1.4370,\n","         -0.8546, -0.2560,  0.8045, -0.3894,  1.6665, -1.7804,  2.0974],\n","        [-2.0571,  1.2638,  3.1616, -0.4684,  0.5263, -1.2635,  0.1659,  0.2038,\n","         -0.5672, -2.4325, -0.5962, -0.3910,  1.3754, -0.4313,  2.0015],\n","        [-2.5599,  0.0816,  3.0350, -0.7852, -0.1924, -1.8373, -0.3019,  0.7400,\n","         -1.4510, -2.0509,  0.5362, -0.3264,  1.6164, -1.4441,  2.6733]])\n","tensor([[-0.8635],\n","        [-0.1362],\n","        [-3.7297],\n","        [-3.0025]])\n"]}],"source":["#your code here \n","hidden_channels = 15\n","#weights\n","weights_hidden_1 = torch.randn(hidden_channels, 2)\n","bias_hidden_1 = torch.randn(hidden_channels,)\n","\n","#number of out nodes, one bias per neuron\n","weight_out_1 = torch.randn(1, hidden_channels)\n","bias_out_1 = torch.randn(1,)\n","\n","#hidden\n","h = feed_forward(X, weights_hidden_1, bias_hidden_1)\n","print(h)\n","\n","#output\n","out = feed_forward(h, weight_out_1, bias_out_1)\n","print(out)"]},{"cell_type":"markdown","metadata":{"id":"jR6ycSoYOSPM"},"source":["- Two hidden layers with 6 neuons respectively."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1642618188139,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"2CmuQdXUOSYy","outputId":"e04359d7-6ebb-4ef3-8362-b0a4e5683c89"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.6857,  1.3479,  1.0660,  0.7283,  0.9960,  1.6666],\n","        [-1.9162,  2.9479,  1.1244, -0.3388,  1.0253,  2.8085],\n","        [-1.0342,  1.7759,  1.6040,  0.0473,  0.7141,  1.0931],\n","        [-1.2647,  3.3759,  1.6624, -1.0199,  0.7434,  2.2350]])\n","tensor([[-0.9004, -0.3883, -0.1305,  0.3576, -0.0177, -1.9505]])\n","tensor([[ 4.5972],\n","        [12.7332],\n","        [ 2.4620],\n","        [10.5981]])\n"]}],"source":["#your code here \n","\n","#weights\n","w1 = torch.randn(6, 2) \n","w2 = torch.randn(6, 6)\n","w3 = torch.randn(1, 6)\n","\n","b1 = torch.randn(6, )\n","b2 = torch.randn(6, )\n","b3 = torch.randn(1, )\n","\n","\n","#number of out nodes, one bias per neuron\n","\n","\n","#hidden, output\n","h1 = feed_forward(X, w1, b1)\n","h2 = feed_forward(h1, w2, b2)\n","out = feed_forward(h2, w3, b3)\n","\n","\n","\n","print(h1)\n","print(w3)\n","print(out)\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1642617954567,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"iLTcMhUeqJ04","outputId":"2ec4b078-5894-4e82-e6a2-eccbc3fc8c84"},"outputs":[{"data":{"text/plain":["torch.Size([2, 6])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["w1.t().shape"]},{"cell_type":"markdown","metadata":{"id":"X9-ptB4BNP3v"},"source":["Conditions:\n","\n","don't worry about `torch.Variable` for this challenge as we don't need to keep track of gradients etc here. however all operations should be constructed from the ground up with `torch.tensor` etc.\n","\n","---\n","\n","Note that your output should be a vector consisting of four predictions\n"]},{"cell_type":"markdown","metadata":{"id":"_fgpx-3hDmVa"},"source":["#Part 3\n","\n","Construct the above feedforward networks with the `nn.Linear` method! but this time let's wrap our output with a Sigmoid output activation for use with `BCEloss` in our upcoming lab.\n","\n","This time around the problem can be solved with an output dimension of one too."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1642617660763,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"S7O-eOibPNaD","outputId":"02c4d340-c293-41b8-fb7f-a635ec9039b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.5273],\n","        [0.6285],\n","        [0.6293],\n","        [0.7203]], grad_fn=<SigmoidBackward0>)\n","tensor([[0.4941],\n","        [0.4382],\n","        [0.3731],\n","        [0.3222]], grad_fn=<SigmoidBackward0>)\n","tensor([[0.4932],\n","        [0.4826],\n","        [0.4562],\n","        [0.4457]], grad_fn=<SigmoidBackward0>)\n","[[0]\n"," [0]\n"," [0]\n"," [0]]\n"]}],"source":["#2 hidden\n","import torch.nn as nn\n","lin1= nn.Linear(2, 2)\n","lin2= nn.Linear(2, 1)\n","\n","hidden = lin1(X)\n","out = lin2(hidden)\n","print(torch.sigmoid(out))\n","\n","\n","#6 hidden\n","lin1= nn.Linear(2, 6)\n","lin2= nn.Linear(6, 1)\n","\n","hidden = lin1(X)\n","out = lin2(hidden)\n","\n","print(torch.sigmoid(out))\n","\n","\n","#multiple hidden\n","lin1= nn.Linear(2, 6)\n","lin2= nn.Linear(6, 6)\n","lin3= nn.Linear(6, 1)\n","\n","hidden = lin1(X)\n","hidden2 = lin2(hidden)\n","\n","out = lin3(hidden2)\n","\n","print(torch.sigmoid(out))\n","print(np.where(torch.sigmoid(out) > 0.5, 1, 0))"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1642618108471,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"Zl81nkTpXJV0","outputId":"c16dd2de-f3cd-443e-d079-8bc54e832eab"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6, 2])\n","Parameter containing:\n","tensor([ 0.2834, -0.5570,  0.2926, -0.0404, -0.0508,  0.4108],\n","       requires_grad=True)\n"]}],"source":["print(lin1.weight.shape)\n","print(lin1.bias)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436,"status":"ok","timestamp":1642617896931,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"Fbgw3E4ppuMv","outputId":"c41114c9-966d-42ad-d5d9-62dec8267f42"},"outputs":[{"data":{"text/plain":["torch.Size([6, 6])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["lin2.weight.t().shape"]},{"cell_type":"markdown","metadata":{"id":"38gj1DnPXJxe"},"source":["# Part 4\n","\n","Last of all go ahead and check out Ian Goodfellow and Yoshua Bengio and Aaron Courville's [Deep Learning](https://www.deeplearningbook.org/), a fantastic overview of the mathematical foundations of Deep Learning.\n","\n","A html copy is available via the link above (purchasing the textbook is also highly recommended) and navigate to **Part II: Modern Practical Deep Networks - Deep Feed Forward Networks**.\n","\n","----\n","\n","\\\n","Read through this chapter and complete the folowing:\n","\n","- Discover the correct weights and bias vector for a 2 neuron single hidden layer network.\n","- Utlize the information you've learnt in either a custom layer to output the correct predictions.\n","- Calculate the loss to demonstrate this.\n","\n","Note that much like the first scenario this can be solved with a nonlinearity in the hidden layer rather than employing sigmoid on output.\n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"KxaGHwh8b2oY"},"outputs":[],"source":["###your code here###\n","\n","w1 = torch.ones((2, 2), dtype=torch.float32)\n","w2 = torch.tensor([1, 2], dtype=torch.float32)\n","\n","c = torch.tensor([0, -1], dtype=torch.float32)\n","b = torch.tensor([0], dtype=torch.float32)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1642603547601,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"HuyQXlOfw0Ft","outputId":"d4d96d06-54b8-4871-c8cf-419b587dc513"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0.],\n","        [0., 1.],\n","        [1., 0.],\n","        [1., 1.]])\n","tensor([0., 1., 1., 0.])\n"]}],"source":["input = X\n","\n","out = XOR_labels\n","\n","print(input)\n","print(out)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":617,"status":"ok","timestamp":1642603595554,"user":{"displayName":"Rhys Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFFxOIFUp5vHAqfmGqBcZuGms-xgg6K-EIz5n=s64","userId":"11848860578168838838"},"user_tz":0},"id":"BnhfvQHfyDg_","outputId":"50e03eef-5f84-4189-800e-052e21bd5879"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-2.,  1.,  1.,  4.])\n"]}],"source":["h1 = feed_forward(input, w1, c)\n","output = feed_forward(h1, w2, b)\n","\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"pytorch_indexe.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
